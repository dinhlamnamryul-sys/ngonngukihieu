import streamlit as st
import cv2
import mediapipe as mp
import math
import numpy as np
import av
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, WebRtcMode

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(
    page_title="Sign.AI - D·ªãch thu·∫≠t Ng√¥n ng·ªØ K√Ω hi·ªáu",
    page_icon="‚úã",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- KH·ªûI T·∫†O SESSION STATE ---
if 'media_bank' not in st.session_state:
    st.session_state.media_bank = []

# --- CSS T√ôY CH·ªàNH ---
st.markdown("""
<style>
    .main { background-color: #fcfdfe; color: #0f172a; }
    div.stButton > button {
        border-radius: 1rem; font-weight: bold; text-transform: uppercase;
    }
</style>
""", unsafe_allow_html=True)

# --- LOGIC X·ª¨ L√ù ·∫¢NH (MEDIAPIPE) ---
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

class HandGestureProcessor(VideoProcessorBase):
    def __init__(self):
        self.hands = mp_hands.Hands(
            max_num_hands=1,
            model_complexity=0, # Gi·∫£m xu·ªëng 0 ƒë·ªÉ ch·∫°y nhanh h∆°n tr√™n web
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.last_letter = ""
        self.stability_counter = 0

    def analyze_hand_gesture(self, landmarks):
        lm = landmarks.landmark
        
        def dist(p1, p2):
            return math.sqrt((lm[p1].x - lm[p2].x)**2 + (lm[p1].y - lm[p2].y)**2)
        
        def is_ext(tip, pip):
            return lm[tip].y < lm[pip].y

        # Logic nh·∫≠n di·ªán
        index_ext = is_ext(8, 6)
        middle_ext = is_ext(12, 10)
        ring_ext = is_ext(16, 14)
        pinky_ext = is_ext(20, 18)

        # Logic mapping k√Ω t·ª± (A, B, D, I, V, O)
        if not index_ext and not middle_ext and not ring_ext and not pinky_ext and lm[4].y < lm[6].y: return "A"
        if index_ext and middle_ext and ring_ext and pinky_ext and dist(8, 12) < 0.04: return "B"
        if index_ext and not middle_ext and not ring_ext and not pinky_ext: return "D"
        if pinky_ext and not index_ext: return "I"
        if index_ext and middle_ext and not ring_ext: return "V"
        if dist(8, 4) < 0.05 and dist(12, 4) < 0.05: return "O"
        
        return ""

    def recv(self, frame):
        # 1. Chuy·ªÉn ƒë·ªïi t·ª´ av.VideoFrame sang numpy array
        img = frame.to_ndarray(format="bgr24")
        
        # 2. X·ª≠ l√Ω ·∫£nh
        img = cv2.flip(img, 1)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.hands.process(img_rgb)
        
        detected_char = ""

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # V·∫Ω khung x∆∞∆°ng
                mp_drawing.draw_landmarks(
                    img, 
                    hand_landmarks, 
                    mp_hands.HAND_CONNECTIONS,
                    mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=4),
                    mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),
                )
                
                # Ph√¢n t√≠ch c·ª≠ ch·ªâ
                detected_char = self.analyze_hand_gesture(hand_landmarks)
                
                if detected_char:
                    # Logic ·ªïn ƒë·ªãnh (Debounce)
                    if detected_char == self.last_letter:
                        self.stability_counter += 1
                    else:
                        self.last_letter = detected_char
                        self.stability_counter = 0

                    # Hi·ªÉn th·ªã ch·ªØ l√™n m√†n h√¨nh
                    cv2.putText(img, f"Ky tu: {detected_char}", (30, 80), 
                                cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 255), 4, cv2.LINE_AA)
                    
                    # Thanh ti·∫øn tr√¨nh
                    bar_width = int((self.stability_counter / 20) * 200)
                    cv2.rectangle(img, (30, 100), (30 + min(bar_width, 200), 120), (0, 255, 0), -1)

        # 3. Tr·∫£ v·ªÅ frame
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# --- UI CH√çNH ---
with st.sidebar:
    st.title("‚úã Sign.AI")
    menu = st.radio("Menu", ["D·ªãch thu·∫≠t AI", "Th∆∞ vi·ªán K√Ω hi·ªáu", "Qu·∫£n tr·ªã Admin"])

if menu == "D·ªãch thu·∫≠t AI":
    st.header("D·ªãch Thu·∫≠t Camera")
    col1, col2 = st.columns([2, 1])
    
    with col1:
        webrtc_streamer(
            key="sign-detection",
            mode=WebRtcMode.SENDRECV,
            video_processor_factory=HandGestureProcessor,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )

    with col2:
        st.info("üí° H∆∞·ªõng d·∫´n: Gi·ªØ y√™n tay kho·∫£ng 1 gi√¢y ƒë·ªÉ h·ªá th·ªëng ch·ªët ch·ªØ c√°i.")
        st.markdown("**C√°c ch·ªØ c√°i h·ªó tr·ª£:** A, B, D, I, V, O")

elif menu == "Th∆∞ vi·ªán K√Ω hi·ªáu":
    st.header("Th∆∞ vi·ªán")
    if st.session_state.media_bank:
        for item in st.session_state.media_bank:
            with st.expander(f"{item['name']}"):
                if item['type'] == 'image':
                    st.image(item['data'])
                else:
                    st.video(item['data'])
    else:
        st.warning("Ch∆∞a c√≥ d·ªØ li·ªáu.")

elif menu == "Qu·∫£n tr·ªã Admin":
    st.header("Upload D·ªØ Li·ªáu")
    with st.form("upload"):
        name = st.text_input("T√™n k√Ω hi·ªáu")
        file = st.file_uploader("File ·∫£nh/video")
        submit = st.form_submit_button("L∆∞u")
        
        if submit and file and name:
            ftype = 'video' if 'video' in file.type else 'image'
            st.session_state.media_bank.append({
                "name": name,
                "type": ftype,
                "data": file.read()
            })
            st.success("ƒê√£ l∆∞u!")
