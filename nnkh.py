import streamlit as st
import cv2
import mediapipe as mp
import av
import numpy as np
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, WebRtcMode
import threading

# --- 1. C·∫§U H√åNH TRANG WEB ---
st.set_page_config(page_title="SIGN.AI - Tr√¨nh th√¥ng d·ªãch", page_icon="‚úã", layout="wide")

# --- 2. CSS ƒê·ªÇ T·∫†O GIAO DI·ªÜN GI·ªêNG ·∫¢NH ---
st.markdown("""
    <style>
    /* T·ªïng th·ªÉ n·ªÅn t·ªëi */
    .stApp {
        background-color: #0e1117;
        color: white;
    }
    
    /* Sidebar */
    [data-testid="stSidebar"] {
        background-color: #161b2c;
        border-right: 1px solid #2d2d44;
    }
    
    /* Header ch√≠nh */
    .main-header {
        font-size: 32px;
        font-weight: bold;
        color: white;
        margin-bottom: 10px;
    }
    
    /* Badge tr·∫°ng th√°i */
    .status-badge {
        background-color: #4b50b0;
        color: white;
        padding: 5px 15px;
        border-radius: 15px;
        font-size: 14px;
        font-weight: bold;
        float: right;
    }

    /* Box k·∫øt qu·∫£ (Gi·ªëng h√¨nh E t√≠m) */
    .result-box-container {
        background-color: #1c2136;
        border: 1px solid #2d2d44;
        border-radius: 15px;
        padding: 20px;
        text-align: center;
        margin-top: 20px;
    }
    
    .result-label {
        color: #8b8da0;
        font-size: 14px;
        text-transform: uppercase;
        letter-spacing: 1px;
        margin-bottom: 10px;
    }
    
    .result-value {
        background-color: #5865F2; /* M√†u t√≠m xanh gi·ªëng Discord/·∫¢nh m·∫´u */
        color: white;
        font-size: 60px;
        font-weight: bold;
        border-radius: 12px;
        padding: 20px;
        display: inline-block;
        min-width: 100px;
        box-shadow: 0 4px 15px rgba(88, 101, 242, 0.4);
    }

    /* T√πy ch·ªânh n√∫t b·∫•m camera */
    button {
        border-radius: 8px !important;
    }
    </style>
""", unsafe_allow_html=True)

# --- 3. X·ª¨ L√ù MEDIAPIPE V√Ä LOGIC NH·∫¨N DI·ªÜN ---
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# Bi·∫øn to√†n c·ª•c ƒë·ªÉ l∆∞u k·∫øt qu·∫£ nh·∫≠n di·ªán (Thread safe)
lock = threading.Lock()
shared_state = {"prediction": "..."}

def recognize_gesture(landmarks):
    """
    H√†m nh·∫≠n di·ªán c·ª≠ ch·ªâ d·ª±a tr√™n t·ªça ƒë·ªô (Rule-based ƒë∆°n gi·∫£n).
    C·∫ßn training model AI th·ª±c s·ª± ƒë·ªÉ nh·∫≠n di·ªán ch√≠nh x√°c 26 ch·ªØ c√°i.
    ƒê√¢y l√† logic demo cho c√°c ch·ªØ c√°i c∆° b·∫£n trong ·∫£nh m·∫´u.
    """
    thumb_tip = landmarks[4].y
    index_tip = landmarks[8].y
    middle_tip = landmarks[12].y
    ring_tip = landmarks[16].y
    pinky_tip = landmarks[20].y
    
    thumb_ip = landmarks[3].y
    index_pip = landmarks[6].y
    
    # Logic nh·∫≠n di·ªán (V√≠ d·ª•)
    # A: N·∫Øm ƒë·∫•m, ng√≥n c√°i √°p s√°t c·∫°nh
    if index_tip > index_pip and middle_tip > landmarks[10].y and ring_tip > landmarks[14].y and pinky_tip > landmarks[18].y:
        return "A"
    
    # B: 4 ng√≥n th·∫≥ng, ng√≥n c√°i g·∫≠p (B√†n tay m·ªü)
    if index_tip < index_pip and middle_tip < landmarks[10].y and ring_tip < landmarks[14].y and pinky_tip < landmarks[18].y and thumb_tip > thumb_ip:
        return "B"

    # V: Ng√≥n tr·ªè v√† gi·ªØa t·∫°o ch·ªØ V
    if index_tip < index_pip and middle_tip < landmarks[10].y and ring_tip > landmarks[14].y and pinky_tip > landmarks[18].y:
        return "V"
        
    # L: Ng√≥n c√°i v√† tr·ªè vu√¥ng g√≥c
    if thumb_tip < landmarks[3].y and index_tip < index_pip and middle_tip > landmarks[10].y:
        return "L"

    # E: (Gi·ªëng h√¨nh m·∫´u) C√°c ng√≥n co l·∫°i, ng√≥n c√°i g·∫≠p d∆∞·ªõi
    # Logic: C√°c ƒë·∫ßu ng√≥n tay ƒë·ªÅu th·∫•p (t·ªça ƒë·ªô y cao) g·∫ßn g√≤ b√†n tay
    if index_tip > index_pip and middle_tip > landmarks[10].y and ring_tip > landmarks[14].y and pinky_tip > landmarks[18].y and thumb_tip > index_tip:
        return "E"

    return ""

class VideoProcessor(VideoTransformerBase):
    def __init__(self):
        self.hands = mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        
        # L·∫≠t ·∫£nh ƒë·ªÉ gi·ªëng g∆∞∆°ng
        img = cv2.flip(img, 1)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        results = self.hands.process(img_rgb)
        
        gesture = ""
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # V·∫Ω khung x∆∞∆°ng tay
                mp_drawing.draw_landmarks(
                    img, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                
                # Nh·∫≠n di·ªán
                gesture = recognize_gesture(hand_landmarks.landmark)
                
                # C·∫≠p nh·∫≠t k·∫øt qu·∫£ v√†o bi·∫øn chung
                with lock:
                    shared_state["prediction"] = gesture if gesture else "..."
        
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# --- 4. B·ªê C·ª§C GIAO DI·ªÜN (LAYOUT) ---

# Sidebar
with st.sidebar:
    st.title("üñêÔ∏è SIGN.AI")
    st.markdown("---")
    st.info("üí° H∆∞·ªõng d·∫´n: ƒê∆∞a tay v√†o khung h√¨nh camera ƒë·ªÉ nh·∫≠n di·ªán ch·ªØ c√°i.")
    st.markdown("### Th∆∞ vi·ªán k√Ω hi·ªáu")
    st.image("https://upload.wikimedia.org/wikipedia/commons/4/47/American_Sign_Language_ASL.svg", caption="B·∫£ng ch·ªØ c√°i tham kh·∫£o")
    
    st.markdown("---")
    st.caption("Developed by Gemini User")

# Main Content
col1, col2 = st.columns([3, 1.5])

with col1:
    st.markdown('<div class="main-header">Tr√¨nh th√¥ng d·ªãch AI <span class="status-badge">H·ªÜ TH·ªêNG ƒêANG CH·∫†Y</span></div>', unsafe_allow_html=True)
    
    # Khu v·ª±c Camera WebRTC
    ctx = webrtc_streamer(
        key="example",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
        video_processor_factory=VideoProcessor,
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True,
    )

with col2:
    st.markdown("<br><br>", unsafe_allow_html=True) # Spacer
    
    # H·ªôp hi·ªÉn th·ªã k·∫øt qu·∫£ Real-time
    st.markdown("""
        <div class="result-box-container">
            <div class="result-label">K·∫æT QU·∫¢ NH·∫¨N DI·ªÜN</div>
            <div class="result-value" id="prediction-placeholder">?</div>
        </div>
    """, unsafe_allow_html=True)
    
    # Ph·∫ßn hi·ªÉn th·ªã vƒÉn b·∫£n th√¥
    st.markdown("<br>", unsafe_allow_html=True)
    st.text_area("VƒÉn b·∫£n th√¥ (Ghi ch√∫):", height=150, placeholder="C√°c k√Ω t·ª± s·∫Ω xu·∫•t hi·ªán t·∫°i ƒë√¢y...")

# --- 5. C∆† CH·∫æ C·∫¨P NH·∫¨T K·∫æT QU·∫¢ T·ª™ THREAD WEBRTC RA UI ---
# Streamlit c·∫ßn reload ƒë·ªÉ update UI, d√πng st_autorefresh ho·∫∑c placeholder loop
import time
placeholder = st.empty()

if ctx.state.playing:
    while True:
        with lock:
            current_pred = shared_state["prediction"]
        
        # C·∫≠p nh·∫≠t tr·ª±c ti·∫øp v√†o h·ªôp HTML b√™n ph·∫£i b·∫±ng JavaScript hack ho·∫∑c hi·ªÉn th·ªã l·∫°i
        # V√¨ Streamlit ch·∫∑n JS tr·ª±c ti·∫øp, ta d√πng markdown ƒë√® l√™n v√πng ƒë√≥
        with col2:
             st.markdown(f"""
                <div class="result-box-container">
                    <div class="result-label">ƒê·ªò ·ªîN ƒê·ªäNH: CAO</div>
                    <div class="result-value">{current_pred}</div>
                </div>
            """, unsafe_allow_html=True)
        
        time.sleep(0.1) # C·∫≠p nh·∫≠t m·ªói 0.1s
